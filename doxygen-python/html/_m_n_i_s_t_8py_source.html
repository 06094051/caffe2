<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Caffe2 - Python API: caffe2/python/tutorials/py_gen/MNIST.py Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="/static/favicon.png" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
<link href="main.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo" width="56"><a href="/"><img alt="Logo" src="Caffe2-with-name-55-tall.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe2 - Python API
   </div>
   <div id="projectbrief">A deep learning, cross platform ML framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Packages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li><a href="/doxygen-c/html/classes.html"><span>C++&#160;API</span></a></li>
      <li><a href="/doxygen-python/html/annotated.html"><span>Python&#160;API</span></a></li>
      <li><a href="https://github.com/caffe2/caffe2"><span>GitHub</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_20697b8f204bdfcab31e6b1a416f3ab8.html">caffe2</a></li><li class="navelem"><a class="el" href="dir_f4fda25fc5253eea1ed54677ae5fa2de.html">python</a></li><li class="navelem"><a class="el" href="dir_97a8e4938fab5a0841bdb9cd8076985e.html">tutorials</a></li><li class="navelem"><a class="el" href="dir_38e1d20e965e77ed1a3671bbc2942034.html">py_gen</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">MNIST.py</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">#########################################################</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment">#</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"># DO NOT EDIT THIS FILE. IT IS GENERATED AUTOMATICALLY. #</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"># PLEASE LOOK INTO THE README FOR MORE INFORMATION.     #</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment">#</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment">#########################################################</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"># coding: utf-8</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"># # MNIST</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"># In this tutorial, we will show you how to train a small Convolutional Neural Network (CNN). We will train the model on the MNIST dataset, which consists of labeled handwritten digits. Each sample from the dataset is a 28x28 pixel grayscale (1 channel) image of a single handwritten digit and the label is an integer from 0 to 9.</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment"># For our model, we will be constructing the [LeNet model](http://yann.lecun.com/exdb/lenet/) with the sigmoid activations replaced with [ReLUs](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf). This tutorial will also give the option to use a simple Multi-layer Perceptron (MLP) model which does not include any convolutional operators. You may select the desired model via the *USE_LENET_MODEL* flag below.</span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"># Before we start, note that we will be using *ModelHelper* class in this tutorial. This class helps us deal with parameter initialization naturally and relieves us from having to maintain the param_init_net and net objects separately.</span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="comment"># Before we start, let&#39;s import the necessities.</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="comment"># In[1]:</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="keyword">from</span> __future__ <span class="keyword">import</span> division</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;<span class="keyword">import</span> os</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="keyword">import</span> shutil</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;<span class="keyword">import</span> operator</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;<span class="keyword">import</span> <a class="code" href="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter.html">caffe2.python.predictor.predictor_exporter</a> <span class="keyword">as</span> pe</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacecaffe2_1_1python.html">caffe2.python</a> <span class="keyword">import</span> (</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;    brew,</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;    core,</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;    model_helper,</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    net_drawer,</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;    optimizer,</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    visualize,</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;    workspace,</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;)</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;<span class="comment"># If you would like to see some really detailed initializations,</span></div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;<span class="comment"># you can change --caffe2_log_level=0 to --caffe2_log_level=-1</span></div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;core.GlobalInit([<span class="stringliteral">&#39;caffe2&#39;</span>, <span class="stringliteral">&#39;--caffe2_log_level=0&#39;</span>])</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;print(<span class="stringliteral">&quot;Necessities imported!&quot;</span>)</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;<span class="comment"># If True, use the LeNet CNN model</span></div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;<span class="comment"># If False, a multilayer perceptron model is used</span></div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;USE_LENET_MODEL = <span class="keyword">True</span></div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;<span class="comment"># ## Data Download</span></div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;<span class="comment"># We will handle the required data download for this tutorial and track statistics during training by setting up a top level local folder named `caffe2_notebooks` in the user&#39;s home directory. For the data download, we need to set up a `tutorial_data` folder, and for the training statistics we setup a `tutorial_files` folder. If you have run this tutorial before you should already have these folders. When the following cell is executed, it will make sure the training and test lmdb databases of MNIST dataset exist in the `tutorial_data` folder. </span></div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;<span class="comment"># In[ ]:</span></div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;<span class="comment"># This section preps your image and test set in a lmdb database</span></div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;<span class="keyword">def </span>DownloadResource(url, path):</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;    <span class="stringliteral">&#39;&#39;&#39;Downloads resources from s3 by url and unzips them to the provided path&#39;&#39;&#39;</span></div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;    <span class="keyword">import</span> requests, zipfile, StringIO</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;    print(<span class="stringliteral">&quot;Downloading... {} to {}&quot;</span>.format(url, path))</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;    r = requests.get(url, stream=<span class="keyword">True</span>)</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;    z = zipfile.ZipFile(StringIO.StringIO(r.content))</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;    z.extractall(path)</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;    print(<span class="stringliteral">&quot;Completed download and extraction.&quot;</span>)</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;    </div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="comment"># Setup the paths for the necessary directories </span></div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;current_folder = os.path.join(os.path.expanduser(<span class="stringliteral">&#39;~&#39;</span>), <span class="stringliteral">&#39;caffe2_notebooks&#39;</span>)</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;data_folder = os.path.join(current_folder, <span class="stringliteral">&#39;tutorial_data&#39;</span>, <span class="stringliteral">&#39;mnist&#39;</span>)</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;root_folder = os.path.join(current_folder, <span class="stringliteral">&#39;tutorial_files&#39;</span>, <span class="stringliteral">&#39;tutorial_mnist&#39;</span>)</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;db_missing = <span class="keyword">False</span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;<span class="comment"># Check if the data folder already exists</span></div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;<span class="keywordflow">if</span> <span class="keywordflow">not</span> os.path.exists(data_folder):</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;    os.makedirs(data_folder)   </div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;    print(<span class="stringliteral">&quot;Your data folder was not found!! This was generated: {}&quot;</span>.format(data_folder))</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="comment"># Check if the training lmdb exists in the data folder</span></div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="keywordflow">if</span> os.path.exists(os.path.join(data_folder,<span class="stringliteral">&quot;mnist-train-nchw-lmdb&quot;</span>)):</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;    print(<span class="stringliteral">&quot;lmdb train db found!&quot;</span>)</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="keywordflow">else</span>:</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;    db_missing = <span class="keyword">True</span></div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;    </div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;<span class="comment"># Check if the testing lmdb exists in the data folder   </span></div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;<span class="keywordflow">if</span> os.path.exists(os.path.join(data_folder,<span class="stringliteral">&quot;mnist-test-nchw-lmdb&quot;</span>)):</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    print(<span class="stringliteral">&quot;lmdb test db found!&quot;</span>)</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;<span class="keywordflow">else</span>:</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;    db_missing = <span class="keyword">True</span></div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;<span class="comment"># Attempt the download of the db if either was missing</span></div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;<span class="keywordflow">if</span> db_missing:</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;    print(<span class="stringliteral">&quot;one or both of the MNIST lmbd dbs not found!!&quot;</span>)</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;    db_url = <span class="stringliteral">&quot;http://download.caffe2.ai/databases/mnist-lmdb.zip&quot;</span></div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;    <span class="keywordflow">try</span>:</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;        DownloadResource(db_url, data_folder)</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;    <span class="keywordflow">except</span> Exception <span class="keyword">as</span> ex:</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;        print(<span class="stringliteral">&quot;Failed to download dataset. Please download it manually from {}&quot;</span>.format(db_url))</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;        print(<span class="stringliteral">&quot;Unzip it and place the two database folders here: {}&quot;</span>.format(data_folder))</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;        <span class="keywordflow">raise</span> ex</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;<span class="comment"># Clean up statistics from any old runs</span></div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;<span class="keywordflow">if</span> os.path.exists(root_folder):</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;    print(<span class="stringliteral">&quot;Looks like you ran this before, so we need to cleanup those old files...&quot;</span>)</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;    shutil.rmtree(root_folder)</div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;    </div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;os.makedirs(root_folder)</div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;workspace.ResetWorkspace(root_folder)</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;print(<span class="stringliteral">&quot;training data folder:&quot;</span> + data_folder)</div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;print(<span class="stringliteral">&quot;workspace root folder:&quot;</span> + root_folder)</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;</div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;<span class="comment"># &gt; If the database wasn&#39;t found in the last step, [download the MNIST lmdb database](https://download.caffe2.ai/databases/mnist-lmdb.zip) or review the [datasets and databases notebook](https://github.com/caffe2/caffe2/blob/master/caffe2/python/tutorials/MNIST_Dataset_and_Databases.ipynb) on how to create the database from the MNIST dataset.</span></div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;<span class="comment"># ## Model Construction</span></div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;<span class="comment"># We will be using the `ModelHelper` class to represent our main model, and use the `brew` module as well as other normal Caffe2 operators to build our model. [`ModelHelper`](https://caffe2.ai/doxygen-python/html/classcaffe2_1_1python_1_1model__helper_1_1_model_helper.html) is a special class which stores a lot of information about parameter initializations, network structure, parameter names and later on mapping to gradients. We will see how it is used with `brew` in other places below.</span></div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;<span class="comment"># To avoid confusion, be aware that *model.MyOperator* is a syntactic sugar for *model.net.MyOperator*, which adds the corresponding *MyOperator* operator to *model.net*.</span></div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;<span class="comment"># **Introduction to brew**</span></div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;<span class="comment"># `brew` is a collection of helper functions designed to simplify the addition of complex logic to our models (for more information click [here](https://caffe2.ai/docs/brew.html)). When we want to add parameter initialization as well as a computation step, for example, `brew` comes in handy. Now, lets explore this in more detail. </span></div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;<span class="comment"># The `brew` module has a set of wrapper functions that automatically separate the parameter intialization and the actual computation into two networks. Under the hood, a `ModelHelper` object has two underlying nets, `param_init_net` and `net`, that keep record of the initialization network and the main network respectively. Also `model.params` keeps track of parameter names.</span></div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;<span class="comment"># **High Level Process**</span></div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;<span class="comment"># For the sake of modularity, we will separate the construction of the model into different parts:</span></div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;<span class="comment">#     (1) The data input part (AddInput function)</span></div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;<span class="comment">#     (2) The main computation part (AddModel function)</span></div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;<span class="comment">#     (3) The training part - adding gradient operators, optimization algorithm, etc. (AddTrainingOperators function)</span></div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;<span class="comment">#     (4) The bookkeeping part, where we just print out statistics for inspection. (AddBookkeepingOperators function)</span></div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;<span class="comment">#   </span></div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;<span class="comment">#   </span></div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;<span class="comment"># ### Add Input</span></div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;<span class="comment"># `AddInput` will load the data from a DB (specifically the lmdbs that were downloaded above). The loaded MNIST data is stored as pixel values in **NCHW** order, so after batching this will give us data with shape `[batch_size, num_channels, width, height]`. More specifically, since our MNIST data are 8-bit (*uint8*) grayscale images of size 28x28px, our data shape as input to the network is `[batch_size, 1, 28, 28]`, and our label type is *int* with shape `[batch_size]`.</span></div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment"># Since we are going to do float computations, we will cast the data to the *float* data type.</span></div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment"># For better numerical stability, instead of representing data in [0, 255] range, we will scale them down to [0, 1].</span></div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment"># Note that we are doing in-place computation for this operator because we don&#39;t need the pre-scaled data.</span></div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;<span class="comment"># Also, when computing the backward pass, we should specify that we will not need the gradient computation for the data preparation part. `StopGradient` does exactly that: in the forward pass it does nothing and in the backward pass all it does is tell the gradient generator &quot;the gradient does not need to pass through here&quot;.</span></div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;<span class="comment"># In[3]:</span></div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;<span class="keyword">def </span>AddInput(model, batch_size, db, db_type):</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;    <span class="comment">### load the data from db - Method 1 using brew</span></div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;    <span class="comment">#data_uint8, label = brew.db_input(</span></div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;    <span class="comment">#    model,</span></div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;    <span class="comment">#    blobs_out=[&quot;data_uint8&quot;, &quot;label&quot;],</span></div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;    <span class="comment">#    batch_size=batch_size,</span></div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;    <span class="comment">#    db=db,</span></div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;    <span class="comment">#    db_type=db_type,</span></div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;    <span class="comment">#)</span></div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;    <span class="comment">### load the data from db - Method 2 using TensorProtosDB</span></div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;    data_uint8, label = model.TensorProtosDBInput(</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;        [], [<span class="stringliteral">&quot;data_uint8&quot;</span>, <span class="stringliteral">&quot;label&quot;</span>], batch_size=batch_size,</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;        db=db, db_type=db_type)</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;    <span class="comment"># cast the data to float</span></div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;    data = model.Cast(data_uint8, <span class="stringliteral">&quot;data&quot;</span>, to=core.DataType.FLOAT)</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;    <span class="comment"># scale data from [0,255] down to [0,1]</span></div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;    data = model.Scale(data, data, scale=float(1./256))</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;    <span class="comment"># don&#39;t need the gradient for the backward pass</span></div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;    data = model.StopGradient(data, data)</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;    <span class="keywordflow">return</span> data, label</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;<span class="comment"># ### MLP Model Definition</span></div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;<span class="comment"># **Note**: This is the model used when the flag *USE_LENET_MODEL=False*</span></div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;<span class="comment"># Now we are going to construct our own model. The input will be our data blob ($X$), and the output will be a vector of length 10 containing the network&#39;s prediction on each of the 10 possible digits in MNIST.</span></div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;<span class="comment"># In this Multi-layer Perceptron (MLP) architecture, the ReLU activation function is going to be used and is defined as:</span></div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;<span class="comment"># $$Relu(x) =\begin{cases}x &amp; if x &gt; 0\\ 0 &amp; otherwise\end{cases}$$</span></div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment"># Recall, each layer of an MLP is just matrix multiplication between some input data and a weight matrix ($W$), then an addition with a bias ($b$).  In machine learning this is accomplished with a fully-connected layer, which is aptly named [FC](https://caffe2.ai/docs/operators-catalogue.html#fc) in Caffe2. The result of this multiplication is then passed to a non-linear activation function (ReLU). The output of each activation function is passed to the next layer as the input data and this process is repeated through the layers as follows:</span></div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;<span class="comment"># $ {layer_1 = Relu(XW_1^T + b_1) } $</span></div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;<span class="comment"># $ {layer_2 = Relu(layer_1W_2^T + b_2) } $</span></div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;<span class="comment"># $ {...} $</span></div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;<span class="comment"># Once the data has propagated through the network in the forward pass, we will use the [Softmax](https://caffe2.ai/docs/operators-catalogue.html#softmax) operator to convert scores for each of the digits to probabilities. As a rule, the scores sum to one (i.e. $p_0 + ... + p_9 = 1.0$) and no single score is greater than or equal to one (i.e. $0 &lt;= p_i &lt;= 1.0$). A more detailed description of MLP can be found [here](http://deeplearning.net/tutorial/mlp.html).</span></div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;<span class="comment"># In this function we are going to use Brew for the first time. Under the hood, when we call `brew.fc(model, layer, ...)` the following happens:</span></div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;<span class="comment"># - The FC operator is added to `model.net` by calling `model.net.FC([layer, W, b], ...)`</span></div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;<span class="comment"># - The initializations for $W$ and $b$ are added to the `model.param_init_net`</span></div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;<span class="comment"># Hopefully, this makes the convenience of brew is clearer! </span></div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;<span class="comment"># In[4]:</span></div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;</div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;<span class="comment"># Function to construct a MLP neural network</span></div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;<span class="comment"># The input &#39;model&#39; is a model helper and &#39;data&#39; is the input data blob&#39;s name</span></div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;<span class="keyword">def </span>AddMLPModel(model, data):</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;    size = 28 * 28 * 1</div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;    sizes = [size, size * 2, size * 2, 10]</div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;    layer = data</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(len(sizes) - 1):</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;        layer = brew.fc(model, layer, <span class="stringliteral">&#39;dense_{}&#39;</span>.format(i), dim_in=sizes[i], dim_out=sizes[i + 1])</div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;        layer = brew.relu(model, layer, <span class="stringliteral">&#39;relu_{}&#39;</span>.format(i))</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;    softmax = brew.softmax(model, layer, <span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;    <span class="keywordflow">return</span> softmax</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    </div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;<span class="comment"># ### LeNet Model Definition</span></div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;<span class="comment"># **Note**: This is the model used when the flag *USE_LENET_MODEL=True*</span></div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;<span class="comment"># Below is another possible (and very powerful) architecture called LeNet. The primary difference from the MLP model is that LeNet is a Convolutional Neural Network (CNN), and therefore uses convolutional layers ([Conv](https://caffe2.ai/docs/operators-catalogue.html#conv)), max pooling layers ([MaxPool](https://caffe2.ai/docs/operators-catalogue.html#maxpool)), [ReLUs](https://caffe2.ai/docs/operators-catalogue.html#relu), *and* fully-connected ([FC](https://caffe2.ai/docs/operators-catalogue.html#fc)) layers. A full explanation of how a CNN works is beyond the scope of this tutorial but here are a few good resources for the curious reader:</span></div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;<span class="comment"># - [Stanford cs231 CNNs for Visual Recognition](http://cs231n.github.io/convolutional-networks/) (**Recommended**)</span></div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;<span class="comment"># - [Explanation of Kernels in Image Processing](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29) </span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;<span class="comment"># - [Convolutional Arithmetic Tutorial](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html)</span></div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;<span class="comment"># Notice, this function also uses Brew. However, this time we add more than just FC and Softmax layers.</span></div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;<span class="comment"># In[5]:</span></div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;<span class="keyword">def </span>AddLeNetModel(model, data):</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;    <span class="stringliteral">&#39;&#39;&#39;</span></div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;<span class="stringliteral">    This part is the standard LeNet model: from data to the softmax prediction.</span></div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;<span class="stringliteral">    </span></div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="stringliteral">    For each convolutional layer we specify dim_in - number of input channels</span></div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;<span class="stringliteral">    and dim_out - number or output channels. Also each Conv and MaxPool layer changes the</span></div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;<span class="stringliteral">    image size. For example, kernel of size 5 reduces each side of an image by 4.</span></div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;<span class="stringliteral">    While when we have kernel and stride sizes equal 2 in a MaxPool layer, it divides</span></div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;<span class="stringliteral">    each side in half.</span></div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;<span class="stringliteral">    &#39;&#39;&#39;</span></div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;    <span class="comment"># Image size: 28 x 28 -&gt; 24 x 24</span></div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;    conv1 = brew.conv(model, data, <span class="stringliteral">&#39;conv1&#39;</span>, dim_in=1, dim_out=20, kernel=5)</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;    <span class="comment"># Image size: 24 x 24 -&gt; 12 x 12</span></div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;    pool1 = brew.max_pool(model, conv1, <span class="stringliteral">&#39;pool1&#39;</span>, kernel=2, stride=2)</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;    <span class="comment"># Image size: 12 x 12 -&gt; 8 x 8</span></div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;    conv2 = brew.conv(model, pool1, <span class="stringliteral">&#39;conv2&#39;</span>, dim_in=20, dim_out=50, kernel=5)</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;    <span class="comment"># Image size: 8 x 8 -&gt; 4 x 4</span></div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;    pool2 = brew.max_pool(model, conv2, <span class="stringliteral">&#39;pool2&#39;</span>, kernel=2, stride=2)</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;    <span class="comment"># 50 * 4 * 4 stands for dim_out from previous layer multiplied by the image size</span></div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;    <span class="comment"># Here, the data is flattened from a tensor of dimension 50x4x4 to a vector of length 50*4*4</span></div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;    fc3 = brew.fc(model, pool2, <span class="stringliteral">&#39;fc3&#39;</span>, dim_in=50 * 4 * 4, dim_out=500)</div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;    relu3 = brew.relu(model, fc3, <span class="stringliteral">&#39;relu3&#39;</span>)</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;    <span class="comment"># Last FC Layer</span></div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;    pred = brew.fc(model, relu3, <span class="stringliteral">&#39;pred&#39;</span>, dim_in=500, dim_out=10)</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;    <span class="comment"># Softmax Layer</span></div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;    softmax = brew.softmax(model, pred, <span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;    </div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;    <span class="keywordflow">return</span> softmax</div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;</div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;<span class="comment"># The `AddModel` function below allows us to easily switch from MLP to LeNet model. Just change `USE_LENET_MODEL` at the very top of the notebook and rerun the whole thing.</span></div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;<span class="comment"># In[6]:</span></div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;<span class="keyword">def </span>AddModel(model, data):</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;    <span class="keywordflow">if</span> USE_LENET_MODEL:</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;        <span class="keywordflow">return</span> AddLeNetModel(model, data)</div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;    <span class="keywordflow">else</span>:</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;        <span class="keywordflow">return</span> AddMLPModel(model, data)</div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;</div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;</div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;<span class="comment"># ### Accuracy Layer</span></div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;<span class="comment"># The `AddAccuracy` function below adds an accuracy operator to the model. It uses the softmax scores and the input training labels (remember this is a supervised learning technique) to report an accuracy score for the current training batch. It is generally **NOT** recommended to get overly excited about accuracy scores during training, as the value gives no indication about the generalization performance, but the training accuracy does give an idea if the model is learning at all. Keeping track of the accuracy through the training process will also let us build a nice plot to summarize the training run.</span></div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;</div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;<span class="comment"># In[7]:</span></div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;</div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;</div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;<span class="keyword">def </span>AddAccuracy(model, softmax, label):</div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Adds an accuracy op to the model&quot;&quot;&quot;</span></div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;    accuracy = brew.accuracy(model, [softmax, label], <span class="stringliteral">&quot;accuracy&quot;</span>)</div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;    <span class="keywordflow">return</span> accuracy</div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;</div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;</div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;<span class="comment"># ### Add Training Operators</span></div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;<span class="comment"># The next function, `AddTrainingOperators`, adds training operators to the model. Please follow inline comments to understand all of the steps. To summarize, this is where we specify how the model computes the *loss* and also the particular *optimization algorithm* and its hyperparameters. In this tutorial, we are going to use a vanilla SGD algorithm, built with the `build_sgd` helper function from the optimizer module. As a result of this function, the model object contains information such as parameter names (`model.param`) and mappings from parameter names to corresponding gradients.</span></div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;</div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;<span class="comment"># In[8]:</span></div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;</div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;</div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;<span class="keyword">def </span>AddTrainingOperators(model, softmax, label):</div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Adds training operators to the model.&quot;&quot;&quot;</span></div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;    <span class="comment"># Compute cross entropy between softmax scores and labels</span></div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;    xent = model.LabelCrossEntropy([softmax, label], <span class="stringliteral">&#39;xent&#39;</span>)</div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;    <span class="comment"># Compute the expected loss</span></div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;    loss = model.AveragedLoss(xent, <span class="stringliteral">&quot;loss&quot;</span>)</div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;    <span class="comment"># Track the accuracy of the model</span></div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;    AddAccuracy(model, softmax, label)</div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;    <span class="comment"># Use the average loss we just computed to add gradient operators to the model</span></div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;    model.AddGradientOperators([loss])</div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;    <span class="comment"># Specify the optimization algorithm</span></div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;    optimizer.build_sgd(</div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;        model,</div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;        base_learning_rate=0.1,</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;        policy=<span class="stringliteral">&quot;step&quot;</span>,</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;        stepsize=1,</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;        gamma=0.999,</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;    )</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;<span class="comment"># ### Add Bookkeeping Operators</span></div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;<span class="comment"># The following function, `AddBookkeepingOperations`, adds a few bookkeeping operators that we can inspect later. These operators do not affect the training procedure: they only collect statistics and print them to files or to logs. Therefore, this is not a necessary step but the bookkeeping will allow us to review the results of training after training has finished.</span></div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;<span class="comment"># In[9]:</span></div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;</div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;<span class="keyword">def </span>AddBookkeepingOperators(model):</div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;This adds a few bookkeeping operators that we can inspect later.</span></div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;<span class="stringliteral">    </span></div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;<span class="stringliteral">    These operators do not affect the training procedure: they only collect</span></div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;<span class="stringliteral">    statistics and prints them to file or to logs.</span></div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span>    </div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;    <span class="comment"># Print basically prints out the content of the blob. to_file=1 routes the</span></div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;    <span class="comment"># printed output to a file. The file is going to be stored under</span></div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;    <span class="comment">#     root_folder/[blob name]</span></div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;    model.Print(<span class="stringliteral">&#39;accuracy&#39;</span>, [], to_file=1)</div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;    model.Print(<span class="stringliteral">&#39;loss&#39;</span>, [], to_file=1)</div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;    <span class="comment"># Summarizes the parameters. Different from Print, Summarize gives some</span></div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;    <span class="comment"># statistics of the parameter, such as mean, std, min and max.</span></div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;    <span class="keywordflow">for</span> param <span class="keywordflow">in</span> model.params:</div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;        model.Summarize(param, [], to_file=1)</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;        model.Summarize(model.param_to_grad[param], [], to_file=1)</div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;    <span class="comment"># Now, if we really want to be verbose, we can summarize EVERY blob</span></div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;    <span class="comment"># that the model produces; it is probably not a good idea, because that</span></div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;    <span class="comment"># is going to take time - summarization do not come for free. For this</span></div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;    <span class="comment"># demo, we will only show how to summarize the parameters and their</span></div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;    <span class="comment"># gradients.</span></div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;</div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;</div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;<span class="comment"># ### Construct the Models</span></div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;<span class="comment"># Now that we have our functions defined, let&#39;s actually create the models for training, testing, and one for simulated deployment. If you are seeing WARNING messages below, do not be alarmed. The functions we established earlier are now going to be executed. Remember the four steps that we are doing:</span></div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;<span class="comment">#     (1) data input  </span></div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="comment">#     (2) main computation</span></div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;<span class="comment">#     (3) training </span></div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;<span class="comment">#     (4) bookkeeping</span></div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;</div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="comment"># In[10]:</span></div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;</div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;</div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;<span class="comment">#### Train Model</span></div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;<span class="comment"># Specify the data will be input in NCHW order</span></div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;<span class="comment">#  (i.e. [batch_size, num_channels, height, width])</span></div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;arg_scope = {<span class="stringliteral">&quot;order&quot;</span>: <span class="stringliteral">&quot;NCHW&quot;</span>}</div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;<span class="comment"># Create the model helper for the train model</span></div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;train_model = model_helper.ModelHelper(name=<span class="stringliteral">&quot;mnist_train&quot;</span>, arg_scope=arg_scope)</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;<span class="comment"># Specify the input is from the train lmdb</span></div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;data, label = AddInput(</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;    train_model, batch_size=64,</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;    db=os.path.join(data_folder, <span class="stringliteral">&#39;mnist-train-nchw-lmdb&#39;</span>),</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;    db_type=<span class="stringliteral">&#39;lmdb&#39;</span>)</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;<span class="comment"># Add the model definition (fc layers, conv layers, softmax, etc.)</span></div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;softmax = AddModel(train_model, data)</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;<span class="comment"># Add training operators, specify loss function and optimization algorithm</span></div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;AddTrainingOperators(train_model, softmax, label)</div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;<span class="comment"># Add bookkeeping operators to save stats from training</span></div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;AddBookkeepingOperators(train_model)</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;<span class="comment">#### Testing model. </span></div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;<span class="comment"># We will set the batch size to 100, so that the testing</span></div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;<span class="comment">#   pass is 100 iterations (10,000 images in total).</span></div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;<span class="comment">#   For the testing model, we need the data input part, the main AddModel</span></div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;<span class="comment">#   part, and an accuracy part. Note that init_params is set False because</span></div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;<span class="comment">#   we will be using the parameters obtained from the train model which will</span></div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;<span class="comment">#   already be in the workspace.</span></div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;test_model = model_helper.ModelHelper(</div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;    name=<span class="stringliteral">&quot;mnist_test&quot;</span>, arg_scope=arg_scope, init_params=<span class="keyword">False</span>)</div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;data, label = AddInput(</div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;    test_model, batch_size=100,</div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;    db=os.path.join(data_folder, <span class="stringliteral">&#39;mnist-test-nchw-lmdb&#39;</span>),</div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;    db_type=<span class="stringliteral">&#39;lmdb&#39;</span>)</div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;softmax = AddModel(test_model, data)</div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;AddAccuracy(test_model, softmax, label)</div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;<span class="comment">#### Deployment model. </span></div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;<span class="comment"># We simply need the main AddModel part.</span></div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;deploy_model = model_helper.ModelHelper(</div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;    name=<span class="stringliteral">&quot;mnist_deploy&quot;</span>, arg_scope=arg_scope, init_params=<span class="keyword">False</span>)</div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;AddModel(deploy_model, <span class="stringliteral">&quot;data&quot;</span>)</div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;</div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;</div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;<span class="comment"># ### Visualize our Progress</span></div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;<span class="comment"># Now, let&#39;s take a look what the training and deploy models look like using the simple graph visualization tool that Caffe2 has. If the following command fails for you, it might be because your machine does not have graphviz installed. You&#39;ll need to install it through the package manager of your choice.</span></div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;<span class="comment"># If the graph looks too small, right click and open the image in a new tab for better inspection.</span></div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;</div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="comment"># In[11]:</span></div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;</div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;</div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;<span class="keyword">from</span> IPython <span class="keyword">import</span> display</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;graph = net_drawer.GetPydotGraph(train_model.net.Proto().op, <span class="stringliteral">&quot;mnist&quot;</span>, rankdir=<span class="stringliteral">&quot;LR&quot;</span>)</div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;display.Image(graph.create_png(), width=800)</div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;</div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;<span class="comment"># Now, the graph above shows everything that is happening in the training phase: the white nodes are the blobs, and the green rectangular nodes are the operators being run. You may have noticed the massive parallel lines like train tracks: these are dependencies from the blobs generated in the forward pass to their backward operators.</span></div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;<span class="comment"># Let&#39;s display the graph in a more minimal way by showing only the necessary dependencies and only showing the operators. If you read carefully, you can see that the left half of the graph is the forward pass, the right half of the graph is the backward pass, and on the very right there are a set of parameter update and summarization operators.</span></div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;</div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;<span class="comment"># In[12]:</span></div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;</div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;graph = net_drawer.GetPydotGraphMinimal(</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;    train_model.net.Proto().op, <span class="stringliteral">&quot;mnist&quot;</span>, rankdir=<span class="stringliteral">&quot;LR&quot;</span>, minimal_dependency=<span class="keyword">True</span>)</div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;display.Image(graph.create_png(), width=800)</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;<span class="comment"># It is important to reiterate here the fact that the ModelHelper class has not executed anything yet. All we have done so far is declare the network, which is basically creating a computation graph specified by the protocol buffers. Here is an example of how we can show a portion of the serialized protocol buffer for the training model&#39;s main network and the parameter initialization network.</span></div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;<span class="comment"># In[13]:</span></div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;</div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;print(<span class="stringliteral">&quot;*******train_model.net.Proto()*******\n&quot;</span>)</div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;print(str(train_model.net.Proto())[:400] + <span class="stringliteral">&#39;\n...&#39;</span>)</div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;print(<span class="stringliteral">&quot;\n*******train_model.param_init_net.Proto()*******\n&quot;</span>)</div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;print(str(train_model.param_init_net.Proto())[:400] + <span class="stringliteral">&#39;\n...&#39;</span>)</div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;</div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;</div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;<span class="comment"># ## Run Training</span></div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;<span class="comment"># With our model built and our data source specified we can now run the training procedure. Please note that this process may take a while to run. Keep an eye on the asterisk (In [\*]) or other IPython indicators that the code block is still running.</span></div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;<span class="comment"># We perform training by just executing our network many times in a row, each time on a fresh batch of inputs. Note, during this process we can fetch values of any blobs in the workspace including accuracy and loss. This allows us to build training plots and periodically monitor the training accuracy and loss. Also, training does not stop once the model has seen all of the data once (1 epoch), rather, it continues until the specified number of iterations is reached.</span></div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;<span class="comment"># When using MLP, model accuracy greatly depends on the random initialization of parameters and the number of training iterations. If your model gets stuck at about 50% accuracy, re-run the notebook, which will start from another random seed and new parameter initialization.</span></div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;</div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;<span class="comment"># In[14]:</span></div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;<span class="comment"># The parameter initialization network only needs to be run once.</span></div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment"># Now all the parameter blobs are initialized in the workspace.</span></div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;workspace.RunNetOnce(train_model.param_init_net)</div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;</div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;<span class="comment"># Creating an actual network as a C++ object in memory.</span></div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;<span class="comment">#   We need this as the object is going to be used a lot</span></div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;<span class="comment">#   so we avoid creating an object every single time it is used.</span></div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment"># overwrite=True allows you to run this cell several times and avoid errors</span></div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;workspace.CreateNet(train_model.net, overwrite=<span class="keyword">True</span>)</div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;</div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;<span class="comment"># Set the iterations number and track the accuracy &amp; loss</span></div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;total_iters = 200</div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;accuracy = np.zeros(total_iters)</div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;loss = np.zeros(total_iters)</div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;</div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="comment"># MAIN TRAINING LOOP!</span></div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;<span class="comment"># Now, we will manually run the network for 200 iterations. </span></div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(total_iters):</div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;    workspace.RunNet(train_model.net)</div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;    accuracy[i] = workspace.blobs[<span class="stringliteral">&#39;accuracy&#39;</span>]</div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;    loss[i] = workspace.blobs[<span class="stringliteral">&#39;loss&#39;</span>]</div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;    <span class="comment"># Check the accuracy and loss every so often</span></div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;    <span class="keywordflow">if</span> i % 25 == 0:</div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;        print(<span class="stringliteral">&quot;Iter: {}, Loss: {}, Accuracy: {}&quot;</span>.format(i,loss[i],accuracy[i]))</div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;<span class="comment"># After the execution is done, let&#39;s plot the values.</span></div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;pyplot.plot(loss, <span class="stringliteral">&#39;b&#39;</span>)</div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;pyplot.plot(accuracy, <span class="stringliteral">&#39;r&#39;)</span></div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;<span class="stringliteral">pyplot.title(&quot;Summary of Training Run&quot;</span>)</div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;pyplot.xlabel(<span class="stringliteral">&quot;Iteration&quot;</span>)</div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;pyplot.legend((<span class="stringliteral">&#39;Loss&#39;</span>, <span class="stringliteral">&#39;Accuracy&#39;</span>), loc=<span class="stringliteral">&#39;upper right&#39;</span>)</div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;<span class="comment"># ## Visualize Results</span></div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment"># Now that our model is trained we can visualize the results of our labors. But first, check out what was printed during the training run above. In the first iteration, the training accuracy for batch 0 was 3.1%, but it quickly converged to mid-nineties by the 100th iteration. The plot also shows this trend. </span></div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;<span class="comment"># This is a good sign!</span></div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;<span class="comment"># In[15]:</span></div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;</div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;<span class="comment">### Let&#39;s look at some of the training data</span></div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;pyplot.figure()</div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;pyplot.title(<span class="stringliteral">&quot;Training Data Sample&quot;</span>)</div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;<span class="comment"># Grab the most recent data blob (i.e. batch) from the workspace</span></div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;data = workspace.FetchBlob(<span class="stringliteral">&#39;data&#39;</span>)</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;<span class="comment"># Use visualize module to show the examples from the last batch that was fed to the model</span></div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;_ = visualize.NCHW.ShowMultiple(data)</div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;</div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;<span class="comment">### Let&#39;s visualize the softmax result</span></div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;pyplot.figure()</div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;pyplot.title(<span class="stringliteral">&#39;Softmax Prediction for the first image above&#39;</span>)</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;pyplot.ylabel(<span class="stringliteral">&#39;Confidence&#39;</span>)</div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;pyplot.xlabel(<span class="stringliteral">&#39;Label&#39;</span>)</div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;<span class="comment"># Grab and visualize the softmax blob for the batch we just visualized. Since batch size</span></div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment">#  is 64, the softmax blob contains 64 vectors, one for each image in the batch. To grab</span></div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;<span class="comment">#  the vector for the first image, we can simply index the fetched softmax blob at zero.</span></div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;softmax = workspace.FetchBlob(<span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;_ = pyplot.plot(softmax[0], <span class="stringliteral">&#39;ro&#39;</span>)</div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;</div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;<span class="comment"># ### Visualize Learned Convolutional Filters</span></div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;<span class="comment"># Note: This is only applicable if LeNet model was used (*USE_LENET_MODEL = True*)</span></div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;<span class="comment"># For convolutional models we can also see how they &quot;think&quot;, i.e. which features they come up with. Instead of fetching learned weights, which can make less sense to a human, we fetch results of convolving those weights over the input. Here, we grab the output feature maps of the first convolutional layer, *conv1*, which are the result of convolving the first layer filters with the most recent training data batch. Note that if this code is rerun after the evaluation phase, the last mini-batch will change, since evaluation and training share the same workspace.</span></div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;<span class="comment"># In[16]:</span></div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;</div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;<span class="keywordflow">if</span> USE_LENET_MODEL:</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;    pyplot.figure()</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;    pyplot.title(<span class="stringliteral">&quot;Conv1 Output Feature Maps for Most Recent Mini-batch&quot;</span>)</div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;    <span class="comment"># Grab the output feature maps of conv1. Change this to conv2 in order to look into the second one.</span></div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;    <span class="comment">#  Remember, early convolutional layers tend to learn human-interpretable features but later conv</span></div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;    <span class="comment">#  layers work with highly-abstract representations. For this reason, it may be harder to understand</span></div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;    <span class="comment">#  features of the later conv layers.</span></div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;    conv = workspace.FetchBlob(<span class="stringliteral">&#39;conv1&#39;</span>)</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;    </div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;    <span class="comment"># We can look into any channel. Think of it as a feature model learned.</span></div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;    <span class="comment"># In this case we look into the 5th channel. Play with other channels to see other features</span></div><div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;    conv = conv[:,[5],:,:]</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;    _ = visualize.NCHW.ShowMultiple(conv)</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;</div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;</div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;<span class="comment"># ## Test Model</span></div><div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;<span class="comment"># Remember that we created a test net? Now that we have trained our model, and our workspace contains the trained model parameters, we can run the test pass on the previously unseen test data to check our generalization performance in the form of a test accuracy statistic. Although *test_model* will be using the parameters obtained from *train_model*, *test_model.param_init_net* must still be run to initialize the input data.</span></div><div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;<span class="comment"># We are only going to run 100 iterations because we have 10,000 test images and our batch size for the test_model is 100. Also, we will report an average of batch-wise accuracy, rather than reporting accuracy scores for each individual image.</span></div><div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;</div><div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;<span class="comment"># In[17]:</span></div><div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;</div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;<span class="comment"># param_init_net here will only create a data reader</span></div><div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;<span class="comment"># Other parameters won&#39;t be re-created because we selected init_params=False before</span></div><div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;workspace.RunNetOnce(test_model.param_init_net)</div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;workspace.CreateNet(test_model.net, overwrite=<span class="keyword">True</span>)</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;</div><div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;<span class="comment"># Testing Loop </span></div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;test_accuracy = np.zeros(100)</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;<span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(100):</div><div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;    <span class="comment"># Run a forward pass of the net on the current batch</span></div><div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;    workspace.RunNet(test_model.net)</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;    <span class="comment"># Collect the batch accuracy from the workspace</span></div><div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;    test_accuracy[i] = workspace.FetchBlob(<span class="stringliteral">&#39;accuracy&#39;</span>)</div><div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;    </div><div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;<span class="comment"># After the execution is done, let&#39;s plot the accuracy values.</span></div><div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;pyplot.plot(test_accuracy, <span class="stringliteral">&#39;r&#39;)</span></div><div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;<span class="stringliteral">pyplot.title(&#39;Accuracy over test batches.&#39;</span>)</div><div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;print(<span class="stringliteral">&#39;test_accuracy: %f&#39;</span> % test_accuracy.mean())</div><div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;</div><div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;</div><div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;<span class="comment"># ## Deploy Model</span></div><div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;<span class="comment"># ### Save the Deploy Model</span></div><div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;<span class="comment"># Let&#39;s save the deploy model with the trained weights and biases to a file. Hopefully, the importance of this step is evident but this will allow us to bring the model up at another time to run more tests rather, than having to train a model from scratch every time. This step must be done right after training when the trained params are still in the workspace.</span></div><div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;</div><div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;<span class="comment"># In[18]:</span></div><div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;</div><div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;</div><div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;<span class="comment"># construct the model to be exported</span></div><div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;<span class="comment"># the inputs/outputs of the model are manually specified.</span></div><div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;pe_meta = pe.PredictorExportMeta(</div><div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;    predict_net=deploy_model.net.Proto(),</div><div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;    parameters=[str(b) <span class="keywordflow">for</span> b <span class="keywordflow">in</span> deploy_model.params], </div><div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;    inputs=[<span class="stringliteral">&quot;data&quot;</span>],</div><div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;    outputs=[<span class="stringliteral">&quot;softmax&quot;</span>],</div><div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;)</div><div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;</div><div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;<span class="comment"># save the model to a file. Use minidb as the file format</span></div><div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;pe.save_to_db(<span class="stringliteral">&quot;minidb&quot;</span>, os.path.join(root_folder, <span class="stringliteral">&quot;mnist_model.minidb&quot;</span>), pe_meta)</div><div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;print(<span class="stringliteral">&quot;Deploy model saved to: &quot;</span> + root_folder + <span class="stringliteral">&quot;/mnist_model.minidb&quot;</span>)</div><div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;</div><div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;</div><div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;<span class="comment"># ### Load a Saved Model</span></div><div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;<span class="comment"># Note: For the sake of the tutorial we will do this here, but this section represents how we can load a model that was trained and saved at another time, in a different place.</span></div><div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;<span class="comment"># Let&#39;s load the saved deploy model from the previous step and rerun our tests for verification.</span></div><div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;</div><div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;<span class="comment"># In[19]:</span></div><div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;</div><div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;</div><div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;<span class="comment"># Grab and display the last data batch used before we scratch the workspace. This purely for our convenience...</span></div><div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;blob = workspace.FetchBlob(<span class="stringliteral">&quot;data&quot;</span>)</div><div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;pyplot.figure()</div><div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;pyplot.title(<span class="stringliteral">&quot;Batch of Testing Data&quot;</span>)</div><div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;_ = visualize.NCHW.ShowMultiple(blob)</div><div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;</div><div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;<span class="comment"># reset the workspace, to make sure the model is actually loaded</span></div><div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;workspace.ResetWorkspace(root_folder)</div><div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;</div><div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;<span class="comment"># verify that all blobs from training are destroyed. </span></div><div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;print(<span class="stringliteral">&quot;The blobs in the workspace after reset: {}&quot;</span>.format(workspace.Blobs()))</div><div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;</div><div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;<span class="comment"># load the predict net</span></div><div class="line"><a name="l00610"></a><span class="lineno">  610</span>&#160;predict_net = pe.prepare_prediction_net(os.path.join(root_folder, <span class="stringliteral">&quot;mnist_model.minidb&quot;</span>), <span class="stringliteral">&quot;minidb&quot;</span>)</div><div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;</div><div class="line"><a name="l00612"></a><span class="lineno">  612</span>&#160;<span class="comment"># verify that blobs are loaded back</span></div><div class="line"><a name="l00613"></a><span class="lineno">  613</span>&#160;print(<span class="stringliteral">&quot;The blobs in the workspace after loading the model: {}&quot;</span>.format(workspace.Blobs()))</div><div class="line"><a name="l00614"></a><span class="lineno">  614</span>&#160;</div><div class="line"><a name="l00615"></a><span class="lineno">  615</span>&#160;<span class="comment"># feed the previously saved data to the loaded model</span></div><div class="line"><a name="l00616"></a><span class="lineno">  616</span>&#160;workspace.FeedBlob(<span class="stringliteral">&quot;data&quot;</span>, blob)</div><div class="line"><a name="l00617"></a><span class="lineno">  617</span>&#160;</div><div class="line"><a name="l00618"></a><span class="lineno">  618</span>&#160;<span class="comment"># predict</span></div><div class="line"><a name="l00619"></a><span class="lineno">  619</span>&#160;workspace.RunNetOnce(predict_net)</div><div class="line"><a name="l00620"></a><span class="lineno">  620</span>&#160;softmax = workspace.FetchBlob(<span class="stringliteral">&quot;softmax&quot;</span>)</div><div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;</div><div class="line"><a name="l00622"></a><span class="lineno">  622</span>&#160;print(<span class="stringliteral">&quot;Shape of softmax: &quot;</span>,softmax.shape)</div><div class="line"><a name="l00623"></a><span class="lineno">  623</span>&#160;</div><div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;<span class="comment"># Quick way to get the top-1 prediction result</span></div><div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;<span class="comment"># Squeeze out the unnecessary axis. This returns a 1-D array of length 10</span></div><div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;<span class="comment"># Get the prediction and the confidence by finding the maximum value and index of maximum value in preds array</span></div><div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;curr_pred, curr_conf = max(enumerate(softmax[0]), key=operator.itemgetter(1))</div><div class="line"><a name="l00628"></a><span class="lineno">  628</span>&#160;print(<span class="stringliteral">&quot;Prediction: &quot;</span>, curr_pred)</div><div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;print(<span class="stringliteral">&quot;Confidence: &quot;</span>, curr_conf)</div><div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;</div><div class="line"><a name="l00631"></a><span class="lineno">  631</span>&#160;<span class="comment"># the first letter should be predicted correctly</span></div><div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;pyplot.figure()</div><div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;pyplot.title(<span class="stringliteral">&#39;Prediction for the first image&#39;</span>)</div><div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;pyplot.ylabel(<span class="stringliteral">&#39;Confidence&#39;</span>)</div><div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;pyplot.xlabel(<span class="stringliteral">&#39;Label&#39;</span>)</div><div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;_ = pyplot.plot(softmax[0], <span class="stringliteral">&#39;ro&#39;</span>)</div><div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;</div><div class="line"><a name="l00638"></a><span class="lineno">  638</span>&#160;</div><div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;<span class="comment"># This concludes the MNIST tutorial. We hope this tutorial highlighted some of Caffe2&#39;s features and how easy it is to create a simple MLP or CNN model.</span></div><div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;</div><div class="ttc" id="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter_html"><div class="ttname"><a href="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter.html">caffe2.python.predictor.predictor_exporter</a></div><div class="ttdef"><b>Definition:</b> <a href="predictor__exporter_8py_source.html#l00001">predictor_exporter.py:1</a></div></div>
<div class="ttc" id="namespacecaffe2_1_1python_html"><div class="ttname"><a href="namespacecaffe2_1_1python.html">caffe2.python</a></div><div class="ttdef"><b>Definition:</b> <a href="python_2____init_____8py_source.html#l00001">__init__.py:1</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Fri Mar 23 2018 13:03:38 for Caffe2 - Python API by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
<div class="footerContainer">
  <div id="footer_wrap" class="wrapper footerWrapper">
    <div class="footerBlocks">
      <div id="fb_oss" class="footerSection fbOpenSourceFooter">
          <svg class="facebookOSSLogoSvg" viewBox="0 0 1133.9 1133.9" x="0px" y="0px" height=50 width=50>
            <g>
              <path class="logoRing outerRing" d="M 498.3 3.7 c 153.6 88.9 307.3 177.7 461.1 266.2 c 7.6 4.4 10.3 9.1 10.3 17.8 c -0.3 179.1 -0.2 358.3 0 537.4 c 0 8.1 -2.4 12.8 -9.7 17.1 c -154.5 88.9 -308.8 178.1 -462.9 267.5 c -9 5.2 -15.5 5.3 -24.6 0.1 c -153.9 -89.2 -307.9 -178 -462.1 -266.8 C 3 838.8 0 833.9 0 825.1 c 0.3 -179.1 0.2 -358.3 0 -537.4 c 0 -8.6 2.6 -13.6 10.2 -18 C 164.4 180.9 318.4 92 472.4 3 C 477 -1.5 494.3 -0.7 498.3 3.7 Z M 48.8 555.3 c 0 79.9 0.2 159.9 -0.2 239.8 c -0.1 10 3 15.6 11.7 20.6 c 137.2 78.8 274.2 157.8 411 237.3 c 9.9 5.7 17 5.7 26.8 0.1 c 137.5 -79.8 275.2 -159.2 412.9 -238.5 c 7.4 -4.3 10.5 -8.9 10.5 -17.8 c -0.3 -160.2 -0.3 -320.5 0 -480.7 c 0 -8.8 -2.8 -13.6 -10.3 -18 C 772.1 218 633.1 137.8 494.2 57.4 c -6.5 -3.8 -11.5 -4.5 -18.5 -0.5 C 336.8 137.4 197.9 217.7 58.8 297.7 c -7.7 4.4 -10.2 9.2 -10.2 17.9 C 48.9 395.5 48.8 475.4 48.8 555.3 Z" />
              <path class="logoRing middleRing" d="M 184.4 555.9 c 0 -33.3 -1 -66.7 0.3 -100 c 1.9 -48 24.1 -86 64.7 -110.9 c 54.8 -33.6 110.7 -65.5 167 -96.6 c 45.7 -25.2 92.9 -24.7 138.6 1 c 54.4 30.6 108.7 61.5 162.2 93.7 c 44 26.5 67.3 66.8 68 118.4 c 0.9 63.2 0.9 126.5 0 189.7 c -0.7 50.6 -23.4 90.7 -66.6 116.9 c -55 33.4 -110.8 65.4 -167.1 96.5 c -43.4 24 -89 24.2 -132.3 0.5 c -57.5 -31.3 -114.2 -64 -170 -98.3 c -41 -25.1 -62.9 -63.7 -64.5 -112.2 C 183.5 621.9 184.3 588.9 184.4 555.9 Z M 232.9 556.3 c 0 29.5 0.5 59.1 -0.1 88.6 c -0.8 39.2 16.9 67.1 50.2 86.2 c 51.2 29.4 102.2 59.2 153.4 88.4 c 31.4 17.9 63.6 18.3 95 0.6 c 53.7 -30.3 107.1 -61.2 160.3 -92.5 c 29.7 -17.5 45 -44.5 45.3 -78.8 c 0.6 -61.7 0.5 -123.5 0 -185.2 c -0.3 -34.4 -15.3 -61.5 -44.9 -79 C 637.7 352.6 583 320.8 527.9 290 c -27.5 -15.4 -57.2 -16.1 -84.7 -0.7 c -56.9 31.6 -113.4 64 -169.1 97.6 c -26.4 15.9 -40.7 41.3 -41.1 72.9 C 232.6 491.9 232.9 524.1 232.9 556.3 Z" />
              <path class="logoRing innerRing" d="M 484.9 424.4 c 69.8 -2.8 133.2 57.8 132.6 132 C 617 630 558.5 688.7 484.9 689.1 c -75.1 0.4 -132.6 -63.6 -132.7 -132.7 C 352.1 485 413.4 421.5 484.9 424.4 Z M 401.3 556.7 c -3.4 37.2 30.5 83.6 83 84.1 c 46.6 0.4 84.8 -37.6 84.9 -84 c 0.1 -46.6 -37.2 -84.4 -84.2 -84.6 C 432.2 472.1 397.9 518.3 401.3 556.7 Z" />
            </g>
          </svg>
        <h2>Facebook Open Source</h2>
      </div>
      <div class="footerSection">
        <a class="footerLink" href="https://code.facebook.com/projects/" target="_blank">Open Source Projects</a>
        <a class="footerLink" href="https://github.com/facebook/" target="_blank">GitHub</a>
        <a class="footerLink" href="https://twitter.com/fbOpenSource" target="_blank">Twitter</a>
      </div>
      <div class="footerSection rightAlign">
        <a class="footerLink" href="https://github.com/caffe2/caffe2" target="_blank">Contribute to this project on GitHub</a>
      </div>
    </div>
  </div>
</div>
<script type="text/javascript" src="/js/jekyll-link-anchors.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '{{ site.gacode }}', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
