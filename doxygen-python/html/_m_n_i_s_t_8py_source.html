<!-- HTML header for doxygen 1.8.14-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Caffe2 - Python API: caffe2/python/tutorials/py_gen/MNIST.py Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="icon" href="/static/favicon.png" type="image/x-icon">
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="stylesheet.css" rel="stylesheet" type="text/css" />
<link href="main.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo" width="56"><a href="/"><img alt="Logo" src="Caffe2-with-name-55-tall.png"/></a></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Caffe2 - Python API
   </div>
   <div id="projectbrief">A deep learning, cross platform ML framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Packages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li><a href="/doxygen-c/html/classes.html"><span>C++&#160;API</span></a></li>
      <li><a href="/doxygen-python/html/annotated.html"><span>Python&#160;API</span></a></li>
      <li><a href="https://github.com/caffe2/caffe2"><span>GitHub</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_20697b8f204bdfcab31e6b1a416f3ab8.html">caffe2</a></li><li class="navelem"><a class="el" href="dir_f4fda25fc5253eea1ed54677ae5fa2de.html">python</a></li><li class="navelem"><a class="el" href="dir_97a8e4938fab5a0841bdb9cd8076985e.html">tutorials</a></li><li class="navelem"><a class="el" href="dir_38e1d20e965e77ed1a3671bbc2942034.html">py_gen</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">MNIST.py</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">#########################################################</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment">#</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"># DO NOT EDIT THIS FILE. IT IS GENERATED AUTOMATICALLY. #</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"># PLEASE LOOK INTO THE README FOR MORE INFORMATION.     #</span></div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment">#</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment">#########################################################</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"># coding: utf-8</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"># # MNIST</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"># In this tutorial, we will show you how to train a small convolutional neural network (a CNN) model. We will be training the model on the MNIST dataset, which consists of labeled handwritten digits. Each sample is a 28x28 picture of a handwritten single digit and the label is a digit from 0 to 9.</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment"># We will be constructing the [LeNet model](http://yann.lecun.com/exdb/lenet/) with the sigmoid activations replaced with [ReLUs](http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf). A flag below will allow us to toggle between the LeNet model and a simple MLP (multilayer perceptron) architectures.</span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"># We will be using ModelHelper - the class that helps us deal with parameter initialization naturally.</span></div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="comment"># First, let&#39;s import the necessities.</span></div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="comment"># In[1]:</span></div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="keyword">import</span> os</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="keyword">import</span> shutil</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="keyword">import</span> <a class="code" href="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter.html">caffe2.python.predictor.predictor_exporter</a> <span class="keyword">as</span> pe</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="keyword">from</span> <a class="code" href="namespacecaffe2_1_1python.html">caffe2.python</a> <span class="keyword">import</span> (</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;    brew,</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;    core,</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;    model_helper,</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;    net_drawer,</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;    optimizer,</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;    visualize,</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;    workspace,</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;)</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;<span class="comment"># If you would like to see some really detailed initializations,</span></div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;<span class="comment"># you can change --caffe2_log_level=0 to --caffe2_log_level=-1</span></div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;core.GlobalInit([<span class="stringliteral">&#39;caffe2&#39;</span>, <span class="stringliteral">&#39;--caffe2_log_level=0&#39;</span>])</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;print(<span class="stringliteral">&quot;Necessities imported!&quot;</span>)</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;<span class="comment"># If True, a more complicated convolutional model is used</span></div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;<span class="comment"># If False, a multilayer perceptron model is used</span></div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;USE_LENET_MODEL = <span class="keyword">True</span></div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;<span class="comment"># We will track statistics during the training time and store these on disk in a local folder. We need to set up a data folder for the data and a root folder for the stats. You should already have these folders, and in the data folder the MNIST dataset should be setup as a lmdb database for both the training set and the test set for this tutorial. </span></div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;<span class="comment"># In[2]:</span></div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;<span class="comment"># This section preps your image and test set in a lmdb database</span></div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;<span class="keyword">def </span>DownloadResource(url, path):</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;    <span class="stringliteral">&#39;&#39;&#39;Downloads resources from s3 by url and unzips them to the provided path&#39;&#39;&#39;</span></div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;    <span class="keyword">import</span> requests, zipfile, StringIO</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;    print(<span class="stringliteral">&quot;Downloading... {} to {}&quot;</span>.format(url, path))</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;    r = requests.get(url, stream=<span class="keyword">True</span>)</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;    z = zipfile.ZipFile(StringIO.StringIO(r.content))</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;    z.extractall(path)</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;    print(<span class="stringliteral">&quot;Completed download and extraction.&quot;</span>)</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;    </div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;    </div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;current_folder = os.path.join(os.path.expanduser(<span class="stringliteral">&#39;~&#39;</span>), <span class="stringliteral">&#39;caffe2_notebooks&#39;</span>)</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;data_folder = os.path.join(current_folder, <span class="stringliteral">&#39;tutorial_data&#39;</span>, <span class="stringliteral">&#39;mnist&#39;</span>)</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;root_folder = os.path.join(current_folder, <span class="stringliteral">&#39;tutorial_files&#39;</span>, <span class="stringliteral">&#39;tutorial_mnist&#39;</span>)</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;db_missing = <span class="keyword">False</span></div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="keywordflow">if</span> <span class="keywordflow">not</span> os.path.exists(data_folder):</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;    os.makedirs(data_folder)   </div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;    print(<span class="stringliteral">&quot;Your data folder was not found!! This was generated: {}&quot;</span>.format(data_folder))</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;<span class="comment"># Look for existing database: lmdb</span></div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;<span class="keywordflow">if</span> os.path.exists(os.path.join(data_folder,<span class="stringliteral">&quot;mnist-train-nchw-lmdb&quot;</span>)):</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;    print(<span class="stringliteral">&quot;lmdb train db found!&quot;</span>)</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;<span class="keywordflow">else</span>:</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;    db_missing = <span class="keyword">True</span></div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;    </div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;<span class="keywordflow">if</span> os.path.exists(os.path.join(data_folder,<span class="stringliteral">&quot;mnist-test-nchw-lmdb&quot;</span>)):</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;    print(<span class="stringliteral">&quot;lmdb test db found!&quot;</span>)</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="keywordflow">else</span>:</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;    db_missing = <span class="keyword">True</span></div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="comment"># attempt the download of the db if either was missing</span></div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;<span class="keywordflow">if</span> db_missing:</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;    print(<span class="stringliteral">&quot;one or both of the MNIST lmbd dbs not found!!&quot;</span>)</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;    db_url = <span class="stringliteral">&quot;http://download.caffe2.ai/databases/mnist-lmdb.zip&quot;</span></div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    <span class="keywordflow">try</span>:</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;        DownloadResource(db_url, data_folder)</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;    <span class="keywordflow">except</span> Exception <span class="keyword">as</span> ex:</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;        print(<span class="stringliteral">&quot;Failed to download dataset. Please download it manually from {}&quot;</span>.format(db_url))</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;        print(<span class="stringliteral">&quot;Unzip it and place the two database folders here: {}&quot;</span>.format(data_folder))</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;        <span class="keywordflow">raise</span> ex</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;<span class="keywordflow">if</span> os.path.exists(root_folder):</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;    print(<span class="stringliteral">&quot;Looks like you ran this before, so we need to cleanup those old files...&quot;</span>)</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;    shutil.rmtree(root_folder)</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;    </div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;os.makedirs(root_folder)</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;workspace.ResetWorkspace(root_folder)</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;print(<span class="stringliteral">&quot;training data folder:&quot;</span> + data_folder)</div><div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;print(<span class="stringliteral">&quot;workspace root folder:&quot;</span> + root_folder)</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;</div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;</div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="comment"># &gt; If the database wasn&#39;t found in the last step, [download the MNIST lmdb database](https://download.caffe2.ai/databases/mnist-lmdb.zip) or review the [datasets and databases notebook](https://github.com/caffe2/caffe2/blob/master/caffe2/python/tutorials/MNIST_Dataset_and_Databases.ipynb) on how to create the database from the MNIST dataset.</span></div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;</div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;<span class="comment"># We will be using the `ModelHelper` class to represent our main model and using `brew` module as well as normal Caffe2 operators to build our model. `ModelHelper` is a special class which stores a lot of information about parameters initialization, their names and later on mapping to gradients. We will see how it is used in `brew` and other places below.</span></div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;<span class="comment"># model.MyOperator is a syntactic sugar for model.net.MyOperator, which adds the corresponding MyOperator operator to model.net.</span></div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;<span class="comment"># `brew` is a collection of helper functions designed to simplify the addition of complex logic to our models. When we want to add parameter initialization as well as a computation step, for example, `brew` comes in handy. Lets explore this in more detail.</span></div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;<span class="comment"># `brew` module has a set of wrapper functions that automatically separate the parameter intialization and the actual computation into two networks. Under the hood, a `ModelHelper` object has two underlying nets, `param_init_net` and `net`, that keep record of the initialization network and the main network respectively. Also model.params keeps track of parameter names.</span></div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;<span class="comment"># For the sake of modularity, we will separate the model to multiple different parts:</span></div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;<span class="comment">#     (1) The data input part (AddInput function)</span></div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;<span class="comment">#     (2) The main computation part (AddModel function)</span></div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;<span class="comment">#     (3) The training part - adding gradient operators, update, etc. (AddTrainingOperators function)</span></div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;<span class="comment">#     (4) The bookkeeping part, where we just print out statistics for inspection. (AddBookkeepingOperators function)</span></div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;<span class="comment"># `AddInput` will load the data from a DB. We store MNIST data in pixel values, so after batching this will give us data with shape `(batch_size, num_channels, width, height)`, in this case `[batch_size, 1, 28, 28]` of data type *uint8* and a label with shape `[batch_size]` of data type *int*.</span></div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;<span class="comment"># Since we are going to do float computations, we will cast the data to the *float* data type.</span></div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;<span class="comment"># For better numerical stability, instead of representing data in [0, 255] range, we will scale them down to [0, 1].</span></div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;<span class="comment"># Note that we are doing in-place computation for this operator: we don&#39;t need the pre-scale data.</span></div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;<span class="comment"># Now, when computing the backward pass, we will not need the gradient computation for the data preparation part. `StopGradient` does exactly that: in the forward pass it does nothing and in the backward pass all it does is to tell the gradient generator &quot;the gradient does not need to pass through here&quot;.</span></div><div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;</div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;<span class="comment"># In[3]:</span></div><div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;</div><div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;<span class="keyword">def </span>AddInput(model, batch_size, db, db_type):</div><div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;    <span class="comment"># load the data</span></div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;    data_uint8, label = brew.db_input(</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;        model,</div><div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;        blobs_out=[<span class="stringliteral">&quot;data_uint8&quot;</span>, <span class="stringliteral">&quot;label&quot;</span>],</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;        batch_size=batch_size,</div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;        db=db,</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;        db_type=db_type,</div><div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;    )</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;    <span class="comment"># cast the data to float</span></div><div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;    data = model.Cast(data_uint8, <span class="stringliteral">&quot;data&quot;</span>, to=core.DataType.FLOAT)</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;    <span class="comment"># scale data from [0,255] down to [0,1]</span></div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;    data = model.Scale(data, data, scale=float(1./256))</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;    <span class="comment"># don&#39;t need the gradient for the backward pass</span></div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;    data = model.StopGradient(data, data)</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;    <span class="keywordflow">return</span> data, label</div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;</div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;<span class="comment"># Now we are going to construct our own model. The input will be our data blob, and the output will be vectors of length 10 containing the network&#39;s prediction on each of the 10 possible digits.</span></div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;<span class="comment"># We are going to use the Multilayer Perceptron (MLP) architecture. The ReLU activation function is going to be used:</span></div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;<span class="comment"># Relu(x) = x if x &gt; 0 else 0</span></div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;<span class="comment"># Each layer of an MLP is just matrix multiplication with a bias plus an activation function. In our case, with ReLU activation, that is:</span></div><div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;<span class="comment"># layer1 = Relu(X * W1^T + b1)</span></div><div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;<span class="comment"># layer2 = Relu(layer1 * W2^T + b2)</span></div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;<span class="comment"># ...</span></div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;<span class="comment"># Ultimately we will use the Softmax operator to convert scores for each of the digits to probabilities. So (p_0 + ... + p_9) = 1.0 and 0 &lt;= p_i &lt;= 1.0. </span></div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;<span class="comment"># There are more detailed MLP explanations online. A good example is [here](http://deeplearning.net/tutorial/mlp.html).</span></div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;<span class="comment"># In this function we are going to use Brew for the second time. Please refer to the explanation given above. When below we call brew.fc(model, layer, ...) under the hood the following happens. FC operator is going to be added to model.net by calling model.net.FC([layer, W, b], ...). Where W and b are the weight and the bias of this fully connected layer (output = layer * W^T + b). Initially, we get W and b by adding their initialization into model.param_init_net. All of these is happening under the hood. You could just use Brew! :) </span></div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;</div><div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;<span class="comment"># In[4]:</span></div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;<span class="keyword">def </span>AddMLPModel(model, data):</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;    size = 28 * 28 * 1</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;    sizes = [size, size * 2, size * 2, 10]</div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;    layer = data</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;    <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(len(sizes) - 1):</div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;        layer = brew.fc(model, layer, <span class="stringliteral">&#39;dense_{}&#39;</span>.format(i), dim_in=sizes[i], dim_out=sizes[i + 1])</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;        layer = model.net.Relu(layer, <span class="stringliteral">&#39;relu_{}&#39;</span>.format(i))</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;    softmax = model.net.Softmax(layer, <span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;    <span class="keywordflow">return</span> softmax</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;    </div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;<span class="comment"># Below is another possible (and much better) architecture called LeNet. This section is optional and you could run and use this tutorial for MLPs without understanding this function.</span></div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;<span class="comment"># It uses convolutional layers. To understand convolution first you could look into [an explanation of kernels in image processing](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29) </span></div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;<span class="comment"># The next step would be to understand convolutions in machine learning. There are also a lot of great resources online. Such as [this one](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html)</span></div><div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;<span class="comment"># This function is also using Brew, this time for adding convolutional layers as well as fully connected ones.</span></div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;<span class="comment"># In[5]:</span></div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;</div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;</div><div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;<span class="keyword">def </span>AddLeNetModel(model, data):</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;    <span class="stringliteral">&#39;&#39;&#39;</span></div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;<span class="stringliteral">    This part is the standard LeNet model: from data to the softmax prediction.</span></div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;<span class="stringliteral">    </span></div><div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;<span class="stringliteral">    For each convolutional layer we specify dim_in - number of input channels</span></div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;<span class="stringliteral">    and dim_out - number or output channels. Also each Conv and MaxPool layer changes the</span></div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;<span class="stringliteral">    image size. For example, kernel of size 5 reduces each side of an image by 4.</span></div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;<span class="stringliteral">    While when we have kernel and stride sizes equal 2 in a MaxPool layer, it divides</span></div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;<span class="stringliteral">    each side in half.</span></div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;<span class="stringliteral">    &#39;&#39;&#39;</span></div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;    <span class="comment"># Image size: 28 x 28 -&gt; 24 x 24</span></div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;    conv1 = brew.conv(model, data, <span class="stringliteral">&#39;conv1&#39;</span>, dim_in=1, dim_out=20, kernel=5)</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;    <span class="comment"># Image size: 24 x 24 -&gt; 12 x 12</span></div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;    pool1 = model.net.MaxPool(conv1, <span class="stringliteral">&#39;pool1&#39;</span>, kernel=2, stride=2)</div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;    <span class="comment"># Image size: 12 x 12 -&gt; 8 x 8</span></div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;    conv2 = brew.conv(model, pool1, <span class="stringliteral">&#39;conv2&#39;</span>, dim_in=20, dim_out=50, kernel=5)</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    <span class="comment"># Image size: 8 x 8 -&gt; 4 x 4</span></div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;    pool2 = model.net.MaxPool(conv2, <span class="stringliteral">&#39;pool2&#39;</span>, kernel=2, stride=2)</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;    <span class="comment"># 50 * 4 * 4 stands for dim_out from previous layer multiplied by the image size</span></div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;    fc3 = brew.fc(model, pool2, <span class="stringliteral">&#39;fc3&#39;</span>, dim_in=50 * 4 * 4, dim_out=500)</div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;    fc3 = model.net.Relu(fc3, <span class="stringliteral">&#39;relu3&#39;</span>)</div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;    pred = brew.fc(model, fc3, <span class="stringliteral">&#39;pred&#39;</span>, 500, 10)</div><div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;    softmax = model.net.Softmax(pred, <span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;    <span class="keywordflow">return</span> softmax</div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;</div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;<span class="comment"># The `AddModel` function below allows us to easily switch from MLP to LeNet model. Just change `USE_LENET_MODEL` at the very top of the notebook and rerun the whole thing.</span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;</div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;<span class="comment"># In[6]:</span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;</div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;<span class="keyword">def </span>AddModel(model, data):</div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;    <span class="keywordflow">if</span> USE_LENET_MODEL:</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;        <span class="keywordflow">return</span> AddLeNetModel(model, data)</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;    <span class="keywordflow">else</span>:</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;        <span class="keywordflow">return</span> AddMLPModel(model, data)</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="comment"># `AddAccuracy` function below adds an accuracy operator to the model. It is not going to be used in training. But will allow us to track accuracy of the model during training and build a nice plot.</span></div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;</div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;<span class="comment"># In[7]:</span></div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;<span class="keyword">def </span>AddAccuracy(model, softmax, label):</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Adds an accuracy op to the model&quot;&quot;&quot;</span></div><div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;    accuracy = model.Accuracy([softmax, label], <span class="stringliteral">&quot;accuracy&quot;</span>)</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;    <span class="keywordflow">return</span> accuracy</div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;</div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;</div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;<span class="comment"># The next function, `AddTrainingOperators`, adds training operators to the model. Please follow inline comments to understand all of the steps. We are going to use `build_sgd` helper function here. You can also build the whole update process yourself. The model object contains all the required information such as parameter names (`model.param`) and a mapping from parameter names to corresponding gradients.</span></div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;</div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;<span class="comment"># In[8]:</span></div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;</div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;</div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;<span class="keyword">def </span>AddTrainingOperators(model, softmax, label):</div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Adds training operators to the model.&quot;&quot;&quot;</span></div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;    xent = model.LabelCrossEntropy([softmax, label], <span class="stringliteral">&#39;xent&#39;</span>)</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;    <span class="comment"># compute the expected loss</span></div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;    loss = model.AveragedLoss(xent, <span class="stringliteral">&quot;loss&quot;</span>)</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;    <span class="comment"># track the accuracy of the model</span></div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;    AddAccuracy(model, softmax, label)</div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;    <span class="comment"># use the average loss we just computed to add gradient operators to the model</span></div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;    model.AddGradientOperators([loss])</div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;    optimizer.build_sgd(</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;        model,</div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;        base_learning_rate=0.1,</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;        policy=<span class="stringliteral">&quot;step&quot;</span>,</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;        stepsize=1,</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;        gamma=0.999,</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;    )</div><div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;</div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;<span class="comment"># The following function, `AddBookkeepingOperations`, adds a few bookkeeping operators that we can inspect later. These operators do not affect the training procedure: they only collect statistics and prints them to file or to logs.</span></div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;<span class="comment"># In[9]:</span></div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;</div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;</div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;<span class="keyword">def </span>AddBookkeepingOperators(model):</div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;This adds a few bookkeeping operators that we can inspect later.</span></div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;<span class="stringliteral">    </span></div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;<span class="stringliteral">    These operators do not affect the training procedure: they only collect</span></div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;<span class="stringliteral">    statistics and prints them to file or to logs.</span></div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span>    </div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;    <span class="comment"># Print basically prints out the content of the blob. to_file=1 routes the</span></div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;    <span class="comment"># printed output to a file. The file is going to be stored under</span></div><div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;    <span class="comment">#     root_folder/[blob name]</span></div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;    model.Print(<span class="stringliteral">&#39;accuracy&#39;</span>, [], to_file=1)</div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;    model.Print(<span class="stringliteral">&#39;loss&#39;</span>, [], to_file=1)</div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;    <span class="comment"># Summarizes the parameters. Different from Print, Summarize gives some</span></div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;    <span class="comment"># statistics of the parameter, such as mean, std, min and max.</span></div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;    <span class="keywordflow">for</span> param <span class="keywordflow">in</span> model.params:</div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;        model.Summarize(param, [], to_file=1)</div><div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;        model.Summarize(model.param_to_grad[param], [], to_file=1)</div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;    <span class="comment"># Now, if we really want to be verbose, we can summarize EVERY blob</span></div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;    <span class="comment"># that the model produces; it is probably not a good idea, because that</span></div><div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;    <span class="comment"># is going to take time - summarization do not come for free. For this</span></div><div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;    <span class="comment"># demo, we will only show how to summarize the parameters and their</span></div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;    <span class="comment"># gradients.</span></div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;</div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;</div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;<span class="comment"># Now, let&#39;s actually create the models for training and testing. If you are seeing WARNING messages below, don&#39;t be alarmed. The functions we established earlier are now going to be executed. Remember the four steps that we&#39;re doing:</span></div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;<span class="comment">#     (1) data input  </span></div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;<span class="comment">#     (2) main computation</span></div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;<span class="comment">#     (3) training </span></div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;<span class="comment">#     (4) bookkeeping</span></div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;<span class="comment">#     </span></div><div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;<span class="comment"># Before we can do the data input though we need to define our training model. We will basically need every piece of the components we defined above. In this example, we&#39;re using NCHW storage order on the mnist_train dataset. </span></div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;</div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;<span class="comment"># In[10]:</span></div><div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;</div><div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;arg_scope = {<span class="stringliteral">&quot;order&quot;</span>: <span class="stringliteral">&quot;NCHW&quot;</span>}</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;train_model = model_helper.ModelHelper(name=<span class="stringliteral">&quot;mnist_train&quot;</span>, arg_scope=arg_scope)</div><div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;data, label = AddInput(</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;    train_model, batch_size=64,</div><div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;    db=os.path.join(data_folder, <span class="stringliteral">&#39;mnist-train-nchw-lmdb&#39;</span>),</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;    db_type=<span class="stringliteral">&#39;lmdb&#39;</span>)</div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;softmax = AddModel(train_model, data)</div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;AddTrainingOperators(train_model, softmax, label)</div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;AddBookkeepingOperators(train_model)</div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;<span class="comment"># Testing model. We will set the batch size to 100, so that the testing</span></div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;<span class="comment"># pass is 100 iterations (10,000 images in total).</span></div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;<span class="comment"># For the testing model, we need the data input part, the main AddModel</span></div><div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;<span class="comment"># part, and an accuracy part. Note that init_params is set False because</span></div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;<span class="comment"># we will be using the parameters obtained from the train model.</span></div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;test_model = model_helper.ModelHelper(</div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;    name=<span class="stringliteral">&quot;mnist_test&quot;</span>, arg_scope=arg_scope, init_params=<span class="keyword">False</span>)</div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;data, label = AddInput(</div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;    test_model, batch_size=100,</div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;    db=os.path.join(data_folder, <span class="stringliteral">&#39;mnist-test-nchw-lmdb&#39;</span>),</div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;    db_type=<span class="stringliteral">&#39;lmdb&#39;</span>)</div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;softmax = AddModel(test_model, data)</div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;AddAccuracy(test_model, softmax, label)</div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;</div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;<span class="comment"># Deployment model. We simply need the main AddModel part.</span></div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;deploy_model = model_helper.ModelHelper(</div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;    name=<span class="stringliteral">&quot;mnist_deploy&quot;</span>, arg_scope=arg_scope, init_params=<span class="keyword">False</span>)</div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;AddModel(deploy_model, <span class="stringliteral">&quot;data&quot;</span>)</div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;<span class="comment"># You may wonder what happens with the param_init_net part of the deploy_model.</span></div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;<span class="comment"># No, we will not use them, since during deployment time we will not randomly</span></div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;<span class="comment"># initialize the parameters, but load the parameters from the db.</span></div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;</div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;</div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;<span class="comment"># Now, let&#39;s take a look what the training and deploy models look like using the simple graph visualization tool that Caffe2 has. If the following command fails for you, it might be because your machine does not have graphviz installed. You&#39;ll need to install it through the package manager of your choice.</span></div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;<span class="comment"># If the graph looks too small, right click and open the image in a new tab for better inspection.</span></div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;</div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;<span class="comment"># In[11]:</span></div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;</div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;</div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="keyword">from</span> IPython <span class="keyword">import</span> display</div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;graph = net_drawer.GetPydotGraph(train_model.net.Proto().op, <span class="stringliteral">&quot;mnist&quot;</span>, rankdir=<span class="stringliteral">&quot;LR&quot;</span>)</div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;display.Image(graph.create_png(), width=800)</div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;</div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;</div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;<span class="comment"># Now, the graph above shows everything that is happening in the training phase: the white nodes are the blobs, and the green rectangular nodes are the operators being run. You may have noticed the massive parallel lines like train tracks: these are dependencies from the blobs generated in the forward pass to their backward operators.</span></div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;<span class="comment"># Let&#39;s display the graph in a more minimal way by showing only the necessary dependencies and only showing the operators. If you read carefully, you can see that the left half of the graph is the forward pass, the right half of the graph is the backward pass, and on the very right there are a set of parameter update and summarization operators.</span></div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;</div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;<span class="comment"># In[12]:</span></div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;</div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;</div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;graph = net_drawer.GetPydotGraphMinimal(</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;    train_model.net.Proto().op, <span class="stringliteral">&quot;mnist&quot;</span>, rankdir=<span class="stringliteral">&quot;LR&quot;</span>, minimal_dependency=<span class="keyword">True</span>)</div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;display.Image(graph.create_png(), width=800)</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;<span class="comment"># Now, when we run the network, one way is to directly run it from Python. Remember as we are running the network, we can periodically pull blobs from the network - Let&#39;s first show how we do this.</span></div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;<span class="comment"># Before that, though, let&#39;s reiterate the fact that the ModelHelper class has not executed anything yet. All it does is declare the network, which is basically creating the protocol buffers. For example, we will show a portion of the serialized protocol buffer for the training model&#39;s main network and the parameter initialization network.</span></div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;</div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;<span class="comment"># In[13]:</span></div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;</div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;print(str(train_model.net.Proto())[:400] + <span class="stringliteral">&#39;\n...&#39;</span>)</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;print(str(train_model.param_init_net.Proto())[:400] + <span class="stringliteral">&#39;\n...&#39;</span>)</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;<span class="comment"># Next we will run the training procedure. Please note that this process will take a while to run. Keep an eye on the asterisk (In [\*]) or other IPython indicators that the code block is still running.</span></div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;<span class="comment"># We perform training by just executing our network many times in a row. Note how during this process we can fetch values of any blobs in the workspace. This allows us to build training plots. </span></div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;<span class="comment"># </span></div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;<span class="comment"># When using MLP, model accuracy greatly depends on the random initialization of parameters. If your model is staying at about 50% accurate, re-run the notebook, which will start from another random seed and initialize the parameters again.</span></div><div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;</div><div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;<span class="comment"># In[14]:</span></div><div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;</div><div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;</div><div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;<span class="comment"># The parameter initialization network only needs to be run once.</span></div><div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;<span class="comment"># Now all the parameter blobs are going to be initialized in the workspace.</span></div><div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;workspace.RunNetOnce(train_model.param_init_net)</div><div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;<span class="comment"># Creating an actual network as a C++ object in memory.</span></div><div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;<span class="comment"># We need this as its going to be used a lot.</span></div><div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;<span class="comment"># So we avoid an object every single time it is used.</span></div><div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160; </div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;<span class="comment"># overwrite=True allows you to run this cell several times and avoid errors</span></div><div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;workspace.CreateNet(train_model.net, overwrite=<span class="keyword">True</span>)</div><div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;</div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;<span class="comment"># Set the iterations number and track the accuracy &amp; loss</span></div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;total_iters = 200</div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;accuracy = np.zeros(total_iters)</div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;loss = np.zeros(total_iters)</div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;</div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;<span class="comment"># Now, we will manually run the network for 200 iterations. </span></div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(total_iters):</div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;    workspace.RunNet(train_model.net)</div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;    accuracy[i] = workspace.blobs[<span class="stringliteral">&#39;accuracy&#39;</span>]</div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;    loss[i] = workspace.blobs[<span class="stringliteral">&#39;loss&#39;</span>]</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;</div><div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;<span class="comment"># After the execution is done, let&#39;s plot the values.</span></div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;pyplot.plot(loss, <span class="stringliteral">&#39;b&#39;</span>)</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;pyplot.plot(accuracy, <span class="stringliteral">&#39;r&#39;)</span></div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;<span class="stringliteral">pyplot.legend((&#39;Loss&#39;</span>, <span class="stringliteral">&#39;Accuracy&#39;</span>), loc=<span class="stringliteral">&#39;upper right&#39;</span>)</div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;</div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;</div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;<span class="comment"># Now we can sample some of the data and predictions. </span></div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;</div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;<span class="comment"># In[15]:</span></div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;<span class="comment"># Let&#39;s look at some of the data.</span></div><div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;pyplot.figure()</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;data = workspace.FetchBlob(<span class="stringliteral">&#39;data&#39;</span>)</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;_ = visualize.NCHW.ShowMultiple(data)</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;pyplot.figure()</div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;softmax = workspace.FetchBlob(<span class="stringliteral">&#39;softmax&#39;</span>)</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;_ = pyplot.plot(softmax[0], <span class="stringliteral">&#39;ro&#39;</span>)</div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;pyplot.title(<span class="stringliteral">&#39;Prediction for the first image&#39;</span>)</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;</div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;</div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;<span class="comment"># For convolutional models we can also see how they &quot;think&quot;, i.e. which features they come up with. Instead of fetching learned weights, which can make less sense to a human, we fetch results of convolving those weights over the input. Note that if this code is rerun after the evaluation phase, the last mini-batch will change, since evaluation and training share the same workspace.</span></div><div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;</div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;<span class="comment"># In[16]:</span></div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;</div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;</div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;<span class="keywordflow">if</span> USE_LENET_MODEL:</div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;    pyplot.figure()</div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;    <span class="comment"># We look into the first conv layer output. Change this to conv2 in order to look into the second one. </span></div><div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;    conv = workspace.FetchBlob(<span class="stringliteral">&#39;conv1&#39;</span>)</div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;    </div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;    <span class="comment"># We can look into any channel. Think of it as a feature model learned.</span></div><div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;    <span class="comment"># In this case we look into the 5th channel. Play with other channels to see other features</span></div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;    conv = conv[:,[5],:,:]</div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;</div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;    _ = visualize.NCHW.ShowMultiple(conv)</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;</div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment"># Remember that we created the test net? We will run the test pass and report the test accuracy here. Note that although test_model will be using the parameters obtained from train_model, test_model.param_init_net must still be run to initialize the input data.</span></div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;<span class="comment"># In this run, we only need to track the accuracy and we&#39;re also only going to run 100 iterations.</span></div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;</div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;<span class="comment"># In[17]:</span></div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;</div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;</div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment"># param_init_net here will only create a data reader</span></div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;<span class="comment"># Other parameters won&#39;t be re-created because we selected init_params=False before</span></div><div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;workspace.RunNetOnce(test_model.param_init_net)</div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;workspace.CreateNet(test_model.net, overwrite=<span class="keyword">True</span>)</div><div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;test_accuracy = np.zeros(100)</div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;<span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(100):</div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;    workspace.RunNet(test_model.net.Proto().name)</div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;    test_accuracy[i] = workspace.FetchBlob(<span class="stringliteral">&#39;accuracy&#39;</span>)</div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="comment"># After the execution is done, let&#39;s plot the values.</span></div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;pyplot.plot(test_accuracy, <span class="stringliteral">&#39;r&#39;)</span></div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="stringliteral">pyplot.title(&#39;Acuracy over test batches.&#39;</span>)</div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;print(<span class="stringliteral">&#39;test_accuracy: %f&#39;</span> % test_accuracy.mean())</div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;</div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;</div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="comment"># Let&#39;s save the deploy model with the trained weights and biases to a file. </span></div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;</div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;<span class="comment"># In[18]:</span></div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;<span class="comment"># construct the model to be exported</span></div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;<span class="comment"># the inputs/outputs of the model are manually specified.</span></div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;pe_meta = pe.PredictorExportMeta(</div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;    predict_net=deploy_model.net.Proto(),</div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;    parameters=[str(b) <span class="keywordflow">for</span> b <span class="keywordflow">in</span> deploy_model.params], </div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;    inputs=[<span class="stringliteral">&quot;data&quot;</span>],</div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;    outputs=[<span class="stringliteral">&quot;softmax&quot;</span>],</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;)</div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;</div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment"># save the model to a file. Use minidb as the file format</span></div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;pe.save_to_db(<span class="stringliteral">&quot;minidb&quot;</span>, os.path.join(root_folder, <span class="stringliteral">&quot;mnist_model.minidb&quot;</span>), pe_meta)</div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;print(<span class="stringliteral">&quot;The deploy model is saved to: &quot;</span> + root_folder + <span class="stringliteral">&quot;/mnist_model.minidb&quot;</span>)</div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;</div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;<span class="comment"># Now we can load the model back and run the prediction to verify it works.</span></div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;<span class="comment"># In[19]:</span></div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;</div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;</div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;<span class="comment"># we retrieve the last input data out and use it in our prediction test before we scratch the workspace</span></div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;blob = workspace.FetchBlob(<span class="stringliteral">&quot;data&quot;</span>)</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;pyplot.figure()</div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;_ = visualize.NCHW.ShowMultiple(blob)</div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;</div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;<span class="comment"># reset the workspace, to make sure the model is actually loaded</span></div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;workspace.ResetWorkspace(root_folder)</div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;<span class="comment"># verify that all blobs are destroyed. </span></div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;print(<span class="stringliteral">&quot;The blobs in the workspace after reset: {}&quot;</span>.format(workspace.Blobs()))</div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;</div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment"># load the predict net</span></div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;predict_net = pe.prepare_prediction_net(os.path.join(root_folder, <span class="stringliteral">&quot;mnist_model.minidb&quot;</span>), <span class="stringliteral">&quot;minidb&quot;</span>)</div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;</div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;<span class="comment"># verify that blobs are loaded back</span></div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;print(<span class="stringliteral">&quot;The blobs in the workspace after loading the model: {}&quot;</span>.format(workspace.Blobs()))</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;</div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;<span class="comment"># feed the previously saved data to the loaded model</span></div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;workspace.FeedBlob(<span class="stringliteral">&quot;data&quot;</span>, blob)</div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;</div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;<span class="comment"># predict</span></div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;workspace.RunNetOnce(predict_net)</div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;softmax = workspace.FetchBlob(<span class="stringliteral">&quot;softmax&quot;</span>)</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;</div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;<span class="comment"># the first letter should be predicted correctly</span></div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;pyplot.figure()</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;_ = pyplot.plot(softmax[0], <span class="stringliteral">&#39;ro&#39;</span>)</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;pyplot.title(<span class="stringliteral">&#39;Prediction for the first image&#39;</span>)</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;</div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;</div><div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;<span class="comment"># This concludes the MNIST tutorial. We hope this tutorial highlighted some of Caffe2&#39;s features and how easy it is to create a simple MLP or CNN model.</span></div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;</div><div class="ttc" id="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter_html"><div class="ttname"><a href="namespacecaffe2_1_1python_1_1predictor_1_1predictor__exporter.html">caffe2.python.predictor.predictor_exporter</a></div><div class="ttdef"><b>Definition:</b> <a href="predictor__exporter_8py_source.html#l00001">predictor_exporter.py:1</a></div></div>
<div class="ttc" id="namespacecaffe2_1_1python_html"><div class="ttname"><a href="namespacecaffe2_1_1python.html">caffe2.python</a></div><div class="ttdef"><b>Definition:</b> <a href="python_2____init_____8py_source.html#l00001">__init__.py:1</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.14-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Fri Mar 16 2018 13:04:02 for Caffe2 - Python API by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
<div class="footerContainer">
  <div id="footer_wrap" class="wrapper footerWrapper">
    <div class="footerBlocks">
      <div id="fb_oss" class="footerSection fbOpenSourceFooter">
          <svg class="facebookOSSLogoSvg" viewBox="0 0 1133.9 1133.9" x="0px" y="0px" height=50 width=50>
            <g>
              <path class="logoRing outerRing" d="M 498.3 3.7 c 153.6 88.9 307.3 177.7 461.1 266.2 c 7.6 4.4 10.3 9.1 10.3 17.8 c -0.3 179.1 -0.2 358.3 0 537.4 c 0 8.1 -2.4 12.8 -9.7 17.1 c -154.5 88.9 -308.8 178.1 -462.9 267.5 c -9 5.2 -15.5 5.3 -24.6 0.1 c -153.9 -89.2 -307.9 -178 -462.1 -266.8 C 3 838.8 0 833.9 0 825.1 c 0.3 -179.1 0.2 -358.3 0 -537.4 c 0 -8.6 2.6 -13.6 10.2 -18 C 164.4 180.9 318.4 92 472.4 3 C 477 -1.5 494.3 -0.7 498.3 3.7 Z M 48.8 555.3 c 0 79.9 0.2 159.9 -0.2 239.8 c -0.1 10 3 15.6 11.7 20.6 c 137.2 78.8 274.2 157.8 411 237.3 c 9.9 5.7 17 5.7 26.8 0.1 c 137.5 -79.8 275.2 -159.2 412.9 -238.5 c 7.4 -4.3 10.5 -8.9 10.5 -17.8 c -0.3 -160.2 -0.3 -320.5 0 -480.7 c 0 -8.8 -2.8 -13.6 -10.3 -18 C 772.1 218 633.1 137.8 494.2 57.4 c -6.5 -3.8 -11.5 -4.5 -18.5 -0.5 C 336.8 137.4 197.9 217.7 58.8 297.7 c -7.7 4.4 -10.2 9.2 -10.2 17.9 C 48.9 395.5 48.8 475.4 48.8 555.3 Z" />
              <path class="logoRing middleRing" d="M 184.4 555.9 c 0 -33.3 -1 -66.7 0.3 -100 c 1.9 -48 24.1 -86 64.7 -110.9 c 54.8 -33.6 110.7 -65.5 167 -96.6 c 45.7 -25.2 92.9 -24.7 138.6 1 c 54.4 30.6 108.7 61.5 162.2 93.7 c 44 26.5 67.3 66.8 68 118.4 c 0.9 63.2 0.9 126.5 0 189.7 c -0.7 50.6 -23.4 90.7 -66.6 116.9 c -55 33.4 -110.8 65.4 -167.1 96.5 c -43.4 24 -89 24.2 -132.3 0.5 c -57.5 -31.3 -114.2 -64 -170 -98.3 c -41 -25.1 -62.9 -63.7 -64.5 -112.2 C 183.5 621.9 184.3 588.9 184.4 555.9 Z M 232.9 556.3 c 0 29.5 0.5 59.1 -0.1 88.6 c -0.8 39.2 16.9 67.1 50.2 86.2 c 51.2 29.4 102.2 59.2 153.4 88.4 c 31.4 17.9 63.6 18.3 95 0.6 c 53.7 -30.3 107.1 -61.2 160.3 -92.5 c 29.7 -17.5 45 -44.5 45.3 -78.8 c 0.6 -61.7 0.5 -123.5 0 -185.2 c -0.3 -34.4 -15.3 -61.5 -44.9 -79 C 637.7 352.6 583 320.8 527.9 290 c -27.5 -15.4 -57.2 -16.1 -84.7 -0.7 c -56.9 31.6 -113.4 64 -169.1 97.6 c -26.4 15.9 -40.7 41.3 -41.1 72.9 C 232.6 491.9 232.9 524.1 232.9 556.3 Z" />
              <path class="logoRing innerRing" d="M 484.9 424.4 c 69.8 -2.8 133.2 57.8 132.6 132 C 617 630 558.5 688.7 484.9 689.1 c -75.1 0.4 -132.6 -63.6 -132.7 -132.7 C 352.1 485 413.4 421.5 484.9 424.4 Z M 401.3 556.7 c -3.4 37.2 30.5 83.6 83 84.1 c 46.6 0.4 84.8 -37.6 84.9 -84 c 0.1 -46.6 -37.2 -84.4 -84.2 -84.6 C 432.2 472.1 397.9 518.3 401.3 556.7 Z" />
            </g>
          </svg>
        <h2>Facebook Open Source</h2>
      </div>
      <div class="footerSection">
        <a class="footerLink" href="https://code.facebook.com/projects/" target="_blank">Open Source Projects</a>
        <a class="footerLink" href="https://github.com/facebook/" target="_blank">GitHub</a>
        <a class="footerLink" href="https://twitter.com/fbOpenSource" target="_blank">Twitter</a>
      </div>
      <div class="footerSection rightAlign">
        <a class="footerLink" href="https://github.com/caffe2/caffe2" target="_blank">Contribute to this project on GitHub</a>
      </div>
    </div>
  </div>
</div>
<script type="text/javascript" src="/js/jekyll-link-anchors.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '{{ site.gacode }}', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
